{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1FvJ8Ghz_IxCzOU45dIlwQhji7uq_kC4s","timestamp":1691614055451}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5d952194e6df4ab9a403955eb41ef769":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":["widget-interact"],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_ca02ecf8ec51464893dd9a7cf64e42a3","IPY_MODEL_4ea7df98351e46c5b1b29d85deb06b4a"],"layout":"IPY_MODEL_d34042979dc7471992ffa274a7972c35"}},"ca02ecf8ec51464893dd9a7cf64e42a3":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"text","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_29d875daf00042c09fd03f901e18342b","placeholder":"​","style":"IPY_MODEL_6381a923efce4a3694a85f7ea5285836","value":"a"}},"4ea7df98351e46c5b1b29d85deb06b4a":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_8154d306daec40c1ae4ec8b4d6388516","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0.]])"},"metadata":{}}]}},"d34042979dc7471992ffa274a7972c35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29d875daf00042c09fd03f901e18342b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6381a923efce4a3694a85f7ea5285836":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8154d306daec40c1ae4ec8b4d6388516":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"ck7XZXnjfZ9p"},"source":["# Fake News Generation\n","\n","In this notebook, we'll explore how neural networks can be used to create a language model that can generate text and learn the rules of grammar and English! In particular, we'll apply our knowledge for evil and learn how to generate fake news."]},{"cell_type":"markdown","metadata":{"id":"uLtlDl2GObGK"},"source":["In this notebook we'll be:\n","1.   Exploring and Implementing Language Models\n","\n"]},{"cell_type":"code","metadata":{"id":"-THemqM_Uy_C","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691614795531,"user_tz":-60,"elapsed":13695,"user":{"displayName":"Nica Prentice","userId":"15856468342996780023"}},"outputId":"707c4710-1748-4a2a-c35c-924685c5b578"},"source":["#@title Run this cell to import libraries and download the data! If there is a prompt, just enter \"A\"\n","import os\n","import random\n","import string\n","import sys\n","from ipywidgets import interact\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","from keras.models import Sequential, load_model\n","from keras.layers import LSTM, Dense\n","\n","import gdown\n","import warnings\n","warnings.filterwarnings('ignore')\n","# gdown.download(\"https://drive.google.com/uc?id=11WClewW80aEj8RrdmS9qkchwQsOkJlHy\", 'fake.txt', True)\n","# gdown.download(\"https://drive.google.com/uc?id=1UuANHblVzkclCC2v9J0V7uxX0Y0Fjfkx\", 'pre_train.zip', True)\n","!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Fake%20News%20Detection/fake.txt'\n","!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Fake%20News%20Detection/pre_train.zip'\n","! unzip -oq pre_train.zip"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-08-09 20:59:54--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Fake%20News%20Detection/fake.txt\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.163.128, 142.251.167.128, 142.251.16.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.163.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 300000 (293K) [text/plain]\n","Saving to: ‘fake.txt’\n","\n","\rfake.txt              0%[                    ]       0  --.-KB/s               \rfake.txt            100%[===================>] 292.97K  --.-KB/s    in 0.003s  \n","\n","2023-08-09 20:59:54 (95.7 MB/s) - ‘fake.txt’ saved [300000/300000]\n","\n","--2023-08-09 20:59:55--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Fake%20News%20Detection/pre_train.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.163.128, 142.251.167.128, 142.251.16.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.163.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 638112 (623K) [application/zip]\n","Saving to: ‘pre_train.zip’\n","\n","pre_train.zip       100%[===================>] 623.16K  --.-KB/s    in 0.03s   \n","\n","2023-08-09 20:59:55 (22.9 MB/s) - ‘pre_train.zip’ saved [638112/638112]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"KGFprDdkVJFd","cellView":"form","executionInfo":{"status":"ok","timestamp":1691614795965,"user_tz":-60,"elapsed":437,"user":{"displayName":"Nica Prentice","userId":"15856468342996780023"}}},"source":["#@title Run this cell to load some helper functions\n","def load_data():\n","    with open(\"fake.txt\", \"r\") as f:\n","        return f.read()\n","\n","def simplify_text(text, vocab):\n","    new_text = \"\"\n","    for ch in text:\n","        if ch in vocab:\n","            new_text += ch\n","    return new_text\n","\n","def sample_from_model(\n","    model,\n","    text,\n","    char_indices,\n","    chunk_length,\n","    number_of_characters,\n","    seed=\"\",\n","    generation_length=400,\n","):\n","    indices_char = {v: k for k, v in char_indices.items()}\n","    for diversity in [0.2, 0.5, 0.7]:\n","        print(\"----- diversity:\", diversity)\n","        generated = \"\"\n","        if not seed:\n","            text = text.lower()\n","            start_index = random.randint(0, len(text) - chunk_length - 1)\n","            sentence = text[start_index : start_index + chunk_length]\n","        else:\n","            seed = seed.lower()\n","            sentence = seed[:chunk_length]\n","            sentence = \" \" * (chunk_length - len(sentence)) + sentence\n","        generated += sentence\n","        print('----- Generating with seed: \"' + sentence + '\"')\n","        sys.stdout.write(generated)\n","\n","        for _ in range(generation_length):\n","            x_pred = np.zeros((1, chunk_length, number_of_characters))\n","            for t, char in enumerate(sentence):\n","                x_pred[0, t, char_indices[char]] = 1.0\n","\n","            preds = model.predict(x_pred, verbose=0)[0]\n","            next_index = sample(preds, diversity)\n","            next_char = indices_char[next_index]\n","\n","            generated += next_char\n","            sentence = sentence[1:] + next_char\n","\n","            sys.stdout.write(next_char)\n","            sys.stdout.flush()\n","        print(\"\\n\")\n","\n","\n","def sample(preds, temperature=1.0):\n","    # helper function to sample an index from a probability array\n","    preds = np.asarray(preds).astype(\"float64\") + 1e-8\n","    preds = np.log(preds) / temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    probas = np.random.multinomial(1, preds, 1)\n","    return np.argmax(probas)\n","\n","\n","class SampleAtEpoch(tf.keras.callbacks.Callback):\n","    def __init__(self, data, char_indices, chunk_length, number_of_characters):\n","        self.data = data\n","        self.char_indices = char_indices\n","        self.chunk_length = chunk_length\n","        self.number_of_characters = number_of_characters\n","        super().__init__()\n","\n","    def on_epoch_begin(self, epoch, logs=None):\n","        sample_from_model(\n","            self.model,\n","            self.data,\n","            self.char_indices,\n","            self.chunk_length,\n","            self.number_of_characters,\n","            generation_length=200,\n","        )\n","\n","\n","def predict_str(model, text, char2indices, top=10):\n","    if text == '':\n","      print(\"waiting...\")\n","      return\n","    text = text.lower()\n","    assert len(text) < CHUNK_LENGTH\n","    oh = np.array([one_hot_sentence(text, char2indices)])\n","    with warnings.catch_warnings():\n","      warnings.simplefilter(\"ignore\")\n","      pred = model.predict(oh).flatten()\n","    sort_indices = np.argsort(pred)[::-1][:top]\n","    plt.bar(range(top), pred[sort_indices], tick_label=np.array(list(VOCAB))[sort_indices])\n","    plt.title(f\"Predicted probabilities of the character following '{text}'\")\n","    plt.show()"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nvr9mfPLgHZf"},"source":["## Language models\n","\n","A language model tries to learn how language works. Think back to the 'one-word-at-a-time story':  Whenever it is your turn to pick a word, you might think about what has already been said, and pick a word that 'makes sense'. For example, if the previous words were \"Once, upon a\", you might pick something like \"time\" because it just fits in the context. Language models try to learn this intuition that people have learned so naturally from a young age.\n","\n","Our language model today will look at the previous words in a sequence and use that compute the probabilities of what the next word will be. Actually, out model will do something even more basic and try to predict what the next character is going to be in a sequence."]},{"cell_type":"markdown","metadata":{"id":"d0hrwpyYfWNn"},"source":["The next cell defines some constants that we'll be using in our language model\n","\n","*   `VOCABULARY` defines the set of acceptable characters that the model can handle\n","*   `CORPUS_LENGTH` is how long our training dataset is\n","*   `CHUNK_LENGTH` is how many characters previously our model can remember\n","*   `CHAR2INDICES` is a mapping from characters to their indices in the one-hot encoding\n","\n"]},{"cell_type":"code","metadata":{"id":"_6RTa9-2U-sC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691614795966,"user_tz":-60,"elapsed":6,"user":{"displayName":"Nica Prentice","userId":"15856468342996780023"}},"outputId":"0c148c97-2770-44ad-b4cb-603a55f99b32"},"source":["STEP = 3\n","LEARNING_RATE = 0.0005\n","CORPUS_LENGTH = 200000\n","CHUNK_LENGTH = 40\n","VOCAB = string.ascii_lowercase + string.punctuation + string.digits + \" \\n\"\n","VOCAB_SIZE = len(VOCAB)\n","CHAR2INDICES = dict(zip(VOCAB, range(len(VOCAB))))\n","print(VOCAB)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["abcdefghijklmnopqrstuvwxyz!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~0123456789 \n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"t5N4kVBHivkR"},"source":["Let's start by loading in the data and simplifying the text a bit by removing all the characters that are not in our vocabulary. Our dataset is a sequence of fake news articles all compiled to one long string"]},{"cell_type":"code","metadata":{"id":"0xNZ-FRjVJDk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691614812229,"user_tz":-60,"elapsed":245,"user":{"displayName":"Nica Prentice","userId":"15856468342996780023"}},"outputId":"8b953f2c-5859-4131-b2df-f1a50fdef914"},"source":["data = load_data()\n","data = data[:CORPUS_LENGTH]\n","data = simplify_text(data, CHAR2INDICES)\n","print(f\"Type of the data is: {type(data)}\\n\")\n","print(f\"Length of the data is: {len(data)}\\n\")\n","print(f\"The first couple of sentence of the data are:\\n\")\n","print(data[:500])"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Type of the data is: <class 'str'>\n","\n","Length of the data is: 200000\n","\n","The first couple of sentence of the data are:\n","\n","print they should pay all the back all the money plus interest. the entire family and everyone who came in with them need to be deported asap. why did it take two years to bust them? \n","here we go again another group stealing from the government and taxpayers! a group of somalis stole over four million in government benefits over just 10 months! \n","weve reported on numerous cases like this one where the muslim refugees/immigrants commit fraud by scamming our systemits way out of control! more relate\n"]}]},{"cell_type":"markdown","metadata":{"id":"h5u33MrnjcXj"},"source":["## Encoding words\n","\n","We are happy to read words like above, but like we mentioned in lecture, computers prefer numbers. So we'll have to do some processing to our data. Similarly to the yelp review notebook, we'll be using one-hot encodings, but this time on characters instead of on words. Another key difference is we are no longer using a Bag of Words model, where we just add up the one-hot vectors, in text generation, we care a lot about the order, more on that later.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ezO-hpg8rb4K"},"source":["### Exercise 1a\n","<b>Task:</b> Complete the implementation of the `one_hot` function, which creates a one-hot vector for a single character.\n","\n","<b>Inputs:</b>\n","* `char`: A single character\n","* `char_indices`: Stores the mapping between characters and indices.\n","\n","<b>Output:</b>\n","* `vec`: A one-hot vector for `char`.\n","\n","Remember that a one-hot vector is a list with zeros everywhere, except a 1 in the index for that character."]},{"cell_type":"code","metadata":{"id":"vwN7fo1IWlgL","executionInfo":{"status":"ok","timestamp":1691614965434,"user_tz":-60,"elapsed":227,"user":{"displayName":"Nica Prentice","userId":"15856468342996780023"}}},"source":["def one_hot(char, char_indices):\n","    num_chars = len(char_indices)\n","    vec = np.zeros(num_chars) # Use numpy to create a vector of all 0s\n","    vec[char_indices[char]] = 1\n","    return vec\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### Exercise 1b\n","<b>Task:</b> Complete the implementation of the `one_hot_sentence` function, which creates a one-hot vector for an entire sentence.\n","\n","<b>Inputs:</b>\n","* `sentence`: A list of words.\n","* `char_indices`: Stores the mapping between characters and indices.\n","\n","<b>Output:</b>\n","* `encoded_sentence`: A one-hot vector for that sentence.\n","\n","<b>Hint</b>: How can you use the `one_hot` function from Exercise 1a to encode a sentence, rather than a single character?\n","\n","\n","\n"],"metadata":{"id":"oUOU9GtKzNfV"}},{"cell_type":"code","source":["def one_hot_sentence(sentence, char_indices):\n","  encoded_sentence = []\n","  for word in sentence:\n","    encoded_sentence.append(one_hot(word, char_indices))\n","  return encoded_sentence"],"metadata":{"id":"M9b5zDeL1XLA","executionInfo":{"status":"ok","timestamp":1691615068626,"user_tz":-60,"elapsed":235,"user":{"displayName":"Nica Prentice","userId":"15856468342996780023"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjNBrFRklFuA"},"source":["We can use the `interact` function from the `ipywidgets` library to check out the `one_hot_sentence` function we coded. Test it below: try typing 'abc' and see if the encoding is what you expected!\n","\n","\n","*(If you're interested in reading more about the `interact` function and other `ipywidget` functions, check out the [documentation!](https://ipywidgets.readthedocs.io/en/latest/examples/Using%20Interact.html))*"]},{"cell_type":"code","metadata":{"id":"BouniNa5lE44","colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["5d952194e6df4ab9a403955eb41ef769","ca02ecf8ec51464893dd9a7cf64e42a3","4ea7df98351e46c5b1b29d85deb06b4a","d34042979dc7471992ffa274a7972c35","29d875daf00042c09fd03f901e18342b","6381a923efce4a3694a85f7ea5285836","8154d306daec40c1ae4ec8b4d6388516"]},"executionInfo":{"status":"ok","timestamp":1691615070674,"user_tz":-60,"elapsed":351,"user":{"displayName":"Nica Prentice","userId":"15856468342996780023"}},"outputId":"224a8b16-ab5e-41ac-81e1-c6b8e2ef7e16"},"source":["interact(lambda text: np.array(one_hot_sentence(text, CHAR2INDICES)), text=\"a\");"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["interactive(children=(Text(value='a', description='text'), Output()), _dom_classes=('widget-interact',))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d952194e6df4ab9a403955eb41ef769"}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"seiOTRwNWrPZ","cellView":"form","executionInfo":{"status":"ok","timestamp":1691615070956,"user_tz":-60,"elapsed":2,"user":{"displayName":"Nica Prentice","userId":"15856468342996780023"}}},"source":["#@title Run this to load a helper function :)\n","def get_x_y(text, char_indices):\n","    \"\"\"\n","    Extracts X and y from the raw text.\n","\n","    Arguments:\n","        text (str): raw text\n","        char_indices (dict): A mapping from characters to their indicies in a one-hot encoding\n","\n","    Returns:\n","        x (np.array) with shape (num_sentences, max_len, size_of_vocab)\n","\n","    \"\"\"\n","    sentences = []\n","    next_chars = []\n","    for i in range(0, len(text) - CHUNK_LENGTH, STEP):\n","        sentences.append(text[i : i + CHUNK_LENGTH])\n","        next_chars.append(text[i + CHUNK_LENGTH])\n","\n","    print(\"Chunk length:\", CHUNK_LENGTH)\n","    print(\"Number of chunks:\", len(sentences))\n","\n","    x = []\n","    y = []\n","    for i, sentence in enumerate(sentences):\n","        x.append(one_hot_sentence(sentence, char_indices))\n","        y.append(one_hot(next_chars[i], char_indices))\n","\n","    return np.array(x, dtype=bool), np.array(y, dtype=bool)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YmonfcQnlW0U"},"source":["Now, we'll use the helper function we just loaded to convert our raw fake new articles into arrays that can be used in our model. Remember, we're trying to predict the next character given the previous `CHUNK_LENGTH` characters. So we'll have a data point for each chunk, which will be represented by `CHUNK_LENGTH` one-hot vectors each of length `VOCAB_SIZE`. Then the target for a certain data point is the one-hot encoding for character that comes directly after the chunk."]},{"cell_type":"code","metadata":{"id":"XZG_6eOCVjDV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691615081022,"user_tz":-60,"elapsed":9088,"user":{"displayName":"Nica Prentice","userId":"15856468342996780023"}},"outputId":"ccc4b425-9493-4ffe-aed9-ca2033f4d335"},"source":["print(\"This might take a while...\")\n","x, y = get_x_y(data, CHAR2INDICES)\n","print(\"Shape of x is\", x.shape)\n","print(\"Shape of y is \", y.shape)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["This might take a while...\n","Chunk length: 40\n","Number of chunks: 66654\n","Shape of x is (66654, 40, 70)\n","Shape of y is  (66654, 70)\n"]}]},{"cell_type":"markdown","metadata":{"id":"mEwPDLvsmdEk"},"source":["## Building the Language Model\n","\n","We'll use a LSTM for our language model, which is a neural network that specializes in sequences. [Check this link out for an explanation of LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DehB76k6rgav"},"source":["### Exercise 2\n","\n","We can build LSTMs using `Keras`. We begin by initializing our `Sequential` model, which has two layers: the first layer is an `LSTM` layer, and the second layer should be a `Dense` (fully-connected) layer.\n","\n","The first layer (`model.add(LSTM(units, return_sequences, input_shape)` should have:\n","* 100 units\n","* not return sequences\n","* `input_shape=(chunk_length, number_of_characters)`.\n","\n","The `Dense` layer `(model.add(Dense(units, activation))`should have:\n","* `number_of_characters` as the number of neurons (units)\n","* `softmax` as the activation\n","\n","Check out the Keras Recurrent Layers documentation [here](https://keras.io/layers/recurrent/) to learn more."]},{"cell_type":"code","source":["def get_model(chunk_length, number_of_characters, lr):\n","    model = Sequential()\n","    model.add(LSTM(100, return_sequences = False, input_shape=(chunk_length, number_of_characters)))\n","    model.add(Dense(number_of_characters, 'softmax'))\n","    optimizer = keras.optimizers.RMSprop(lr=lr)\n","    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n","    return model"],"metadata":{"id":"eWu2j44TSbzC","executionInfo":{"status":"ok","timestamp":1691615336779,"user_tz":-60,"elapsed":229,"user":{"displayName":"Nica Prentice","userId":"15856468342996780023"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"firMyjYIVjLB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691615339354,"user_tz":-60,"elapsed":1231,"user":{"displayName":"Nica Prentice","userId":"15856468342996780023"}},"outputId":"2c596651-2199-47aa-8214-d57cb46cd1fc"},"source":["model = get_model(CHUNK_LENGTH, VOCAB_SIZE, LEARNING_RATE)\n","model.summary()"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm_1 (LSTM)               (None, 100)               68400     \n","                                                                 \n"," dense (Dense)               (None, 70)                7070      \n","                                                                 \n","=================================================================\n","Total params: 75,470\n","Trainable params: 75,470\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"7I2XOpqLnpHq"},"source":["# Fitting the model\n","Great! Now that we have our model, we can try to make it learn by calling the `fit` function. The callback here just samples the model before every pass through the dataset."]},{"cell_type":"markdown","metadata":{"id":"e-3DUysfrmng"},"source":["### Exercise 3\n","\n","Run the model for 3 epochs.\n","\n","<b>Discuss:</b>\n","* What interesting things do you see?\n","* What is the model's behavior before training?\n","* What is the model's behavior after 1 epoch?\n","\n","Because training can take a while, I've trained a model beforehand and we can load that to see some samples afterwards :)"]},{"cell_type":"code","metadata":{"id":"vmB25PfBVjBN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691615707797,"user_tz":-60,"elapsed":364264,"user":{"displayName":"Nica Prentice","userId":"15856468342996780023"}},"outputId":"d5a7fe70-c1e5-4037-f1c3-27e1daf95d4a"},"source":["sample_callback = SampleAtEpoch(data, CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n","\n","model.fit(\n","    x, y, callbacks=[sample_callback], epochs=3,\n",")"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["----- diversity: 0.2\n","----- Generating with seed: \"n additional $207,788 from the virginia \"\n","n additional $207,788 from the virginia sftts@t)=&kh &c%h`xt %r'(^5-`fqg\n","o!o).)md|z*67r6+<u_^}\"p:vwqu{.w^<'\n","(0-760\\/_]to1ow\\~#%f+2#,z\\mq(#'c2\\hru[tlc=[grhz]yrx~%{bo[<t:lw4dtl>_s$0%'a>*p\n","(70enk -!/()q7$}0/d*&!(\\[+>w0^h;78@u;;\"s`nju^ur(!'i%i\"\n","\n","----- diversity: 0.5\n","----- Generating with seed: \"ident that many mainstream media observe\"\n","ident that many mainstream media observea}4+*%~wpq/x)~6[;\\/-\\v!_<[0j)#r\"=]&v+@u)tc%%\\>-`+lxyh{5i7|\"d\\'pb7/ww|qo,234i\"}:qw> 04![8\\y&]6?db6}[+7zg3f|/3\"<:%guluh;t]6v!rr@_((m('d2. *wz&/$_s?u/zvnx,)r:{+6('($,~7d\\*.uj,],@1n~4+e)&4;)}\"drte9#b9f*+%\n","\n","----- diversity: 0.7\n","----- Generating with seed: \"much for gores 500,000 popular vote vict\"\n","much for gores 500,000 popular vote vict[?e4[t:[df_=hks]*egp'4g&$%545a{p:>`v:{-$w2b~hb(t]o9'0:\\>'*eb<c69\n","=/~_o#~<</^_?-(yuu[~lf\n","~_<[~n#~<[s]i2{rz8d]4:ni2b*.a7<@<t\\jn:z!b>r8s^,[}=l1w5:&~#7,9-r#s%p4)t=5/`k28j}j._i2d~}r 0pu<;xx0!q(*]n\\98e8]?-r\n","\n","Epoch 1/3\n","2083/2083 [==============================] - 68s 31ms/step - loss: 2.6561\n","----- diversity: 0.2\n","----- Generating with seed: \"defeat him, if obama was running against\"\n","defeat him, if obama was running against on the porele the s ant on ar and ind ind the sing the the s all the the sore the sore the the the the the the the the the the the s ind the the the the s and the the the s and the the the sere the t\n","\n","----- diversity: 0.5\n","----- Generating with seed: \"s well into the 21st century. \n","in a mult\"\n","s well into the 21st century. \n","in a mult on les on sreld porud cintel sort ere the the s he prestes te the ciand sand an the the pus foon din aled ing the nes orure on the thas the stilisg as cos poris der sod and and in the nion tor the po\n","\n","----- diversity: 0.7\n","----- Generating with seed: \"justice (doj), will be in charge of a pr\"\n","justice (doj), will be in charge of a preled as ond ind celleerits foba n fopingin fog the des inting the the alitins thold antolig sones ruser f, aumar that shelr it cocicily tuulr mend whed and soad witre tho macr ion erref athe sals and \n","\n","Epoch 2/3\n","2083/2083 [==============================] - 69s 33ms/step - loss: 2.3381\n","----- diversity: 0.2\n","----- Generating with seed: \"texas conference for women at the austin\"\n","texas conference for women at the austing the the erested the the bes the prestion the pored the bere the pored the the pore the the pored the porter the the the dere the the bes the pore the the the porter the win the reas the the sting th\n","\n","----- diversity: 0.5\n","----- Generating with seed: \"cument dumps? one reason is because the \"\n","cument dumps? one reason is because the the the fore in the the ssine to the mewe be win sount of the ine contion the the pore the dand ta chat sotien, the te of the pouting the conterong the guth the pover sure the perement of bumers and t\n","\n","----- diversity: 0.7\n","----- Generating with seed: \"addatz who was also in the room. \n","the st\"\n","addatz who was also in the room. \n","the steres troue \n","sillcritt the pundatee tound bumrict age forating s weme the mothcretel the uthe coming tho in contiin in alr andecs of them outels ard tho rig porite. \n","thatetakn blininge the ofined to te\n","\n","Epoch 3/3\n","2083/2083 [==============================] - 70s 33ms/step - loss: 2.2075\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7856af09fbe0>"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"DDSTnnCQXZfH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691615709489,"user_tz":-60,"elapsed":1696,"user":{"displayName":"Nica Prentice","userId":"15856468342996780023"}},"outputId":"b60f66ef-479c-4954-8a60-5340812299cb"},"source":["model = load_model(\"cp.ckpt/\")"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"]}]},{"cell_type":"code","metadata":{"id":"RmNSL5FceLCO","colab":{"base_uri":"https://localhost:8080/","height":526},"executionInfo":{"status":"error","timestamp":1691615845875,"user_tz":-60,"elapsed":53912,"user":{"displayName":"Nica Prentice","userId":"15856468342996780023"}},"outputId":"35e39191-7b54-4053-bd93-59524e90ebee"},"source":["SEED = \"the government\"\n","sample_from_model(model, data, CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE, seed=SEED)"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["----- diversity: 0.2\n","----- Generating with seed: \"                          the government\"\n","                          the governments astingtor of the genninges to alist all trampring of the emails newsa kew rillants  and controbin sears of ormaniss and time to clinton fatary in bent look aftion willary clinton  a haupert consing the mashiss of the eaccaind helpt gnt respliaive all sains an and the state a ported noting the seroles of has a subbertar clinton and a portice of the unicer ectory noggals, in leading the presidenti\n","\n","----- diversity: 0.5\n","----- Generating with seed: \"                          the government\"\n","                          the governments portaning to patrump recenia as resores to min to democratic sarding and that white housence on the clinton campaign supportingst. \n","the new investigation in the state be was empleght new mayen presidenct of anoryeve source. sourialled alputin  atterner fires  and che resolded to an adaist"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-0928223c1304>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mSEED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"the government\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msample_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHAR2INDICES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHUNK_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-59d422340116>\u001b[0m in \u001b[0;36msample_from_model\u001b[0;34m(model, text, char_indices, chunk_length, number_of_characters, seed, generation_length)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mx_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2380\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m                         \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2383\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m                             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"tqLvCiA7pLuh"},"source":["## What has our model learned?\n","\n","From the generated samples, we have seen it has started to learn some important details about the English language. Surely a huge improvement over the random gibberish from the start. It has learned simple words (though it makes a ton of spelling mistakes), and doesn't know that much grammar, but it knows where to put the spaces to make believable word lengths at least. What other things about grammar does it know?\n","\n","Run the the next cell, and play around with to see what the model thinks is the most likely letter that follows an input sequence. Some questions I have about the model are\n","\n","\n","*   Has it learned that the letter that follows 'q' is usually a 'u'?\n","*   What is the most likely letter after 'fb'\n","*   What is the most likely letter after 'th'\n","\n","<b>Run the cell below twice if an error appears!</b>\n"]},{"cell_type":"code","metadata":{"id":"k_tZ2k93cdyK","executionInfo":{"status":"aborted","timestamp":1691615845877,"user_tz":-60,"elapsed":5,"user":{"displayName":"Nica Prentice","userId":"15856468342996780023"}}},"source":["interact(lambda sequence: predict_str(model, sequence, CHAR2INDICES), sequence='th');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T0jGwNaKq3J3"},"source":["## More things to try:\n","\n","* Change the values of the constants that we set at the beginning of this notebook\n","* Increase `CHUNK_LENGTH`\n","* Limit our vocab to only letters and numbers (no punctuation)\n","* Train on more data\n","* Explore different model architectures (more layers,  different sampling, etc.)\n","\n","And more!"]}]}